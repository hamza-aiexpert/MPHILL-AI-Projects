{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Syed Muhammad Hamza  (MSAI-F22-M010)\n",
    "# Task Assigned is VIVA as Project Improvements\n",
    "#### => Audio record and play button\n",
    "#### => Realtime audio recording and prediction \n",
    "#### => True and predicted label for Activity Monitoring "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8007391ba90c0787"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Please run all testing cells (don't run training cells all models are trained already)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45ff0715282a69de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Activity Monitoring"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10b03cff-05c2-486c-958f-e6a0638207e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90260781b7445f3b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "#For Loading Data\n",
    "def load_data(data_set):\n",
    "        # Load or define the labels\n",
    "\n",
    "        file_names = {\n",
    "            'training': [\n",
    "                'trainLabels.npy',\n",
    "                'trainAccelerometer.npy',\n",
    "                'trainGravity.npy',\n",
    "                'trainGyroscope.npy',\n",
    "                'trainJinsAccelerometer.npy',\n",
    "                'trainJinsGyroscope.npy',\n",
    "                'trainLinearAcceleration.npy',\n",
    "                'trainMagnetometer.npy',\n",
    "                'trainMSAccelerometer.npy',\n",
    "                'trainMSGyroscope.npy'\n",
    "            ],\n",
    "            'testing': [\n",
    "                'testLabels.npy',\n",
    "                'testAccelerometer.npy',\n",
    "                'testGravity.npy',\n",
    "                'testGyroscope.npy',\n",
    "                'testJinsAccelerometer.npy',\n",
    "                'testJinsGyroscope.npy',\n",
    "                'testLinearAcceleration.npy',\n",
    "                'testMagnetometer.npy',\n",
    "                'testMSAccelerometer.npy',\n",
    "                'testMSGyroscope.npy'\n",
    "            ]\n",
    "        }\n",
    "        data = []\n",
    "        for file_name in file_names[data_set]:\n",
    "            file_path = data_dir + data_set + '/' + file_name\n",
    "            loaded_data = np.load(file_path)\n",
    "            data.append(loaded_data)\n",
    "            print(loaded_data.shape, \"\\t\", file_name)\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T15:27:51.160652900Z",
     "start_time": "2023-11-15T15:27:50.784102200Z"
    }
   },
   "id": "d87a49e3b92c1b32"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2284,) \t trainLabels.npy\n",
      "(2284, 800, 3) \t trainAccelerometer.npy\n",
      "(2284, 800, 3) \t trainGravity.npy\n",
      "(2284, 800, 3) \t trainGyroscope.npy\n",
      "(2284, 80, 3) \t trainJinsAccelerometer.npy\n",
      "(2284, 80, 3) \t trainJinsGyroscope.npy\n",
      "(2284, 800, 3) \t trainLinearAcceleration.npy\n",
      "(2284, 200, 3) \t trainMagnetometer.npy\n",
      "(2284, 268, 3) \t trainMSAccelerometer.npy\n",
      "(2284, 268, 3) \t trainMSGyroscope.npy\n",
      "(2288,) \t testLabels.npy\n",
      "(2288, 800, 3) \t testAccelerometer.npy\n",
      "(2288, 800, 3) \t testGravity.npy\n",
      "(2288, 800, 3) \t testGyroscope.npy\n",
      "(2288, 80, 3) \t testJinsAccelerometer.npy\n",
      "(2288, 80, 3) \t testJinsGyroscope.npy\n",
      "(2288, 800, 3) \t testLinearAcceleration.npy\n",
      "(2288, 200, 3) \t testMagnetometer.npy\n",
      "(2288, 268, 3) \t testMSAccelerometer.npy\n",
      "(2288, 268, 3) \t testMSGyroscope.npy\n",
      "Cross-validation scores: [0.74835886 0.75492341 0.78118162 0.75273523 0.75657895]\n",
      "Mean CV score: 0.7587556144189798\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.7592997811816192\n",
      "Validation Precision: 0.7895745795146925\n",
      "Validation F1 Score: 0.7562808340377086\n",
      "Validation Recall: 0.7592997811816192\n",
      "Validation Confusion Matrix:\n",
      " [[11  0  0 ...  0  0  0]\n",
      " [ 0 10  0 ...  0  0  0]\n",
      " [ 0  1  2 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ...  3  0  0]\n",
      " [ 0  0  0 ...  0  9  0]\n",
      " [ 0  0  0 ...  0  0  2]]\n",
      "\n",
      "\n",
      "Test Accuracy: 0.645541958041958\n",
      "Test Precision: 0.6559626860558772\n",
      "Test F1 Score: 0.6387366704482325\n",
      "Test Recall: 0.645541958041958\n",
      "Test Confusion Matrix:\n",
      " [[39  0  0 ...  0  0  0]\n",
      " [ 0 49  1 ...  0  0  0]\n",
      " [ 0 12 14 ...  1  2  1]\n",
      " ...\n",
      " [ 0  1  2 ...  9  0  0]\n",
      " [ 0  0  0 ...  0 37  0]\n",
      " [ 0  0  0 ...  0  0 28]]\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "[[0.9995438]]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Result: male, Probabilities::: Male: 99.95   Female: 0.05\n",
      "Cross-validation scores: [0.75929978 0.77899344 0.80306346 0.78118162 0.76535088]\n",
      "Mean CV score: 0.7775778340819225\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.7614879649890591\n",
      "Validation Precision: 0.7890291024317281\n",
      "Validation F1 Score: 0.755617227259835\n",
      "Validation Recall: 0.7614879649890591\n",
      "Validation Confusion Matrix:\n",
      " [[11  0  0 ...  0  0  0]\n",
      " [ 0 12  0 ...  0  0  0]\n",
      " [ 0  0  3 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  5  0  0]\n",
      " [ 0  0  0 ...  0  8  0]\n",
      " [ 0  0  0 ...  0  0  3]]\n",
      "\n",
      "\n",
      "Test Accuracy: 0.659527972027972\n",
      "Test Precision: 0.6700804074340642\n",
      "Test F1 Score: 0.6525496205262945\n",
      "Test Recall: 0.659527972027972\n",
      "Test Confusion Matrix:\n",
      " [[39  0  0 ...  0  0  0]\n",
      " [ 0 48  1 ...  0  0  0]\n",
      " [ 0 11 18 ...  0  2  1]\n",
      " ...\n",
      " [ 1  0  1 ... 12  0  0]\n",
      " [ 0  0  0 ...  0 37  0]\n",
      " [ 0  0  0 ...  0  0 29]]\n",
      "Cross-validation scores: [0.38074398 0.3785558  0.33916849 0.33260394 0.3245614 ]\n",
      "Mean CV score: 0.3511267227148835\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.3347921225382932\n",
      "Validation Precision: 0.5304883417569446\n",
      "Validation F1 Score: 0.3342383339365394\n",
      "Validation Recall: 0.3347921225382932\n",
      "Validation Confusion Matrix:\n",
      " [[6 0 0 ... 0 0 0]\n",
      " [0 6 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 2 ... 0 5 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "\n",
      "Test Accuracy: 0.34615384615384615\n",
      "Test Precision: 0.43837112969940006\n",
      "Test F1 Score: 0.33414659957600207\n",
      "Test Recall: 0.34615384615384615\n",
      "Test Confusion Matrix:\n",
      " [[36  0  0 ...  0  0  0]\n",
      " [ 0 20 15 ...  0  1  0]\n",
      " [ 1  3 14 ...  0  2  1]\n",
      " ...\n",
      " [ 3  1  0 ...  3  0  0]\n",
      " [ 0  0  4 ...  0 20  0]\n",
      " [ 0  0  0 ...  0  0 11]]\n",
      "Cross-validation scores: [0.65645514 0.60612691 0.63238512 0.66520788 0.62280702]\n",
      "Mean CV score: 0.636596414449691\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.6564551422319475\n",
      "Validation Precision: 0.68333670341329\n",
      "Validation F1 Score: 0.6524297543581116\n",
      "Validation Recall: 0.6564551422319475\n",
      "Validation Confusion Matrix:\n",
      " [[10  0  0 ...  0  0  0]\n",
      " [ 0 10  1 ...  0  0  0]\n",
      " [ 0  0  3 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  3  0  0]\n",
      " [ 0  0  0 ...  0  8  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n",
      "\n",
      "Test Accuracy: 0.5480769230769231\n",
      "Test Precision: 0.5499908272856283\n",
      "Test F1 Score: 0.5414329700412914\n",
      "Test Recall: 0.5480769230769231\n",
      "Test Confusion Matrix:\n",
      " [[39  0  0 ...  0  0  0]\n",
      " [ 0 43  2 ...  0  2  0]\n",
      " [ 1  6 15 ...  1  1  0]\n",
      " ...\n",
      " [ 1  2  1 ... 10  0  0]\n",
      " [ 0  0  1 ...  0 28  0]\n",
      " [ 0  0  0 ...  0  0 24]]\n"
     ]
    }
   ],
   "source": [
    "#Main Class for training and testing\n",
    "class ActivityMonitor:\n",
    "    def __init__(self, train_data, test_data, train_labels, test_labels):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.train_labels = train_labels\n",
    "        self.test_labels = test_labels\n",
    "        self.path_save_model = \"ActivityMonitoring/TrainedModels/\"\n",
    "        \n",
    "    def normalize_data(self, data):\n",
    "        reshaped_data = np.reshape(data, (-1, data.shape[2]))\n",
    "        mean = np.mean(data, axis=0)\n",
    "        std = np.std(data, axis=0)\n",
    "        normalized_data = (data - mean) / std\n",
    "        normalized_data = np.reshape(normalized_data, data.shape)\n",
    "        return normalized_data\n",
    "\n",
    "    def resample_data(self, data, target_frequency):\n",
    "        current_frequency = data.shape[1] / 4.0\n",
    "        resampled_data = signal.resample(data, int(data.shape[1] * target_frequency / current_frequency), axis=1)\n",
    "        return resampled_data\n",
    "\n",
    "    def extract_features(self, data):\n",
    "        mean = np.mean(data, axis=1)\n",
    "        std = np.std(data, axis=1)\n",
    "        min_val = np.min(data, axis=1)\n",
    "        max_val = np.max(data, axis=1)\n",
    "        median = np.median(data, axis=1)\n",
    "        kurt = kurtosis(data, axis=1)\n",
    "        skewness = skew(data, axis=1)\n",
    "        features = np.concatenate([mean, std, min_val, max_val, median, kurt, skewness], axis=1)\n",
    "        return features\n",
    "\n",
    "    def process_data(self, data):\n",
    "        sensor_frequencies = {\n",
    "            1: 200,  # Accelerometer\n",
    "            2: 200,  # Gravity\n",
    "            3: 200,  # Gyroscope\n",
    "            4: 20,   # JinsAccelerometer\n",
    "            5: 20,   # JinsGyroscope\n",
    "            6: 200,  # LinearAcceleration\n",
    "            7: 50,   # Magnetometer\n",
    "            8: 67,   # MSAccelerometer\n",
    "            9: 67    # MSGyroscope\n",
    "        }\n",
    "        processed_data = []\n",
    "        for i, d in enumerate(data[1:], start=1):\n",
    "            if i in sensor_frequencies:\n",
    "                target_frequency = 200\n",
    "                resampled_data = self.resample_data(d, sensor_frequencies[i])\n",
    "                normalized_data = self.normalize_data(resampled_data)\n",
    "                extracted_features = self.extract_features(normalized_data)\n",
    "                processed_data.append(extracted_features)\n",
    "            else:\n",
    "                processed_data.append(d)\n",
    "        processed_data.insert(0, data[0])\n",
    "        return processed_data\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        normalized_train = self.process_data(self.train_data)\n",
    "        normalized_test = self.process_data(self.test_data)\n",
    "        train_labels = normalized_train[0]\n",
    "        self.train_data = np.concatenate(normalized_train[1:], axis=1).reshape(len(train_labels), -1)\n",
    "        test_labels = normalized_test[0]\n",
    "        self.test_data = np.concatenate(normalized_test[1:], axis=1).reshape(len(test_labels), -1)\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def calculate_evaluation_metrics(self, true_labels, predicted_labels, set_name):\n",
    "        acc = accuracy_score(true_labels, predicted_labels)\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "        precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "        recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "        print(\"\\n\")\n",
    "        print(set_name, \"Accuracy:\", acc)\n",
    "        print(set_name, \"Precision:\", precision)\n",
    "        print(set_name, \"F1 Score:\", f1)\n",
    "        print(set_name, \"Recall:\", recall)\n",
    "        print(set_name, \"Confusion Matrix:\\n\", cm)\n",
    "        return acc, cm, f1, precision, recall\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------ \n",
    "    #Random Forest Classifier\n",
    "    def random_forest_classifier(self):\n",
    "        classifier = RandomForestClassifier()\n",
    "        cv_scores = cross_val_score(classifier, self.train_data, self.train_labels, cv=5)\n",
    "        print(\"Cross-validation scores:\", cv_scores)\n",
    "        print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.train_data, self.train_labels, test_size=0.2, random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        joblib.dump(classifier,self.path_save_model + \"random_forest_classifier.joblib\")\n",
    "        val_metrics = self.calculate_evaluation_metrics(y_val, val_predictions, \"Validation\")\n",
    "        test_predictions = classifier.predict(self.test_data)\n",
    "        test_metrics = self.calculate_evaluation_metrics(self.test_labels, test_predictions, \"Test\")\n",
    "        return classifier\n",
    "        \n",
    "    #Finding Hyperparameters Random Forest Grid\n",
    "    # def random_forest_grid_search(self):\n",
    "    #     param_grid = {\n",
    "    #         'n_estimators': [100, 200, 300],\n",
    "    #         'max_depth': [None, 5, 10],\n",
    "    #         'min_samples_split': [2, 5, 10]\n",
    "    #     }\n",
    "    #     classifier = RandomForestClassifier()\n",
    "    #     grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "    #     grid_search.fit(self.train_data, self.train_labels)\n",
    "    #     best_classifier = grid_search.best_estimator_\n",
    "    #     print(\"Selected Hyperparameters:\", best_classifier.get_params(), \"\\n\\n\")\n",
    "    #     cv_scores = cross_val_score(best_classifier, self.train_data, self.train_labels, cv=5)\n",
    "    #     print(\"Cross-validation scores:\", cv_scores)\n",
    "    #     print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "    #     X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "    #     best_classifier.fit(self.train_data, self.train_labels)\n",
    "    #     val_predictions = best_classifier.predict(self.X_val)\n",
    "    #     joblib.dump(best_classifier,self.path_save_model + \"random_forest_grid_search.joblib\")\n",
    "    #     val_metrics = self.calculate_evaluation_metrics(self.y_val, val_predictions, \"Validation\")\n",
    "    #     test_predictions = best_classifier.predict(self.test_data)\n",
    "    #     test_metrics = self.calculate_evaluation_metrics(self.test_labels, test_predictions, \"Test\")\n",
    "    #     return best_classifier\n",
    "        \n",
    "    #Random Forest Grid with trained hyperparameters\n",
    "    def random_forest_grid_search_hyperparameters(self):\n",
    "        classifier = RandomForestClassifier(\n",
    "            bootstrap=True,\n",
    "            ccp_alpha=0.0,\n",
    "            class_weight=None,\n",
    "            criterion='gini',\n",
    "            max_depth=None,\n",
    "            # max_features='auto',\n",
    "            max_leaf_nodes=None,\n",
    "            max_samples=None,\n",
    "            min_impurity_decrease=0.0,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=300,\n",
    "            n_jobs=None,\n",
    "            oob_score=False,\n",
    "            random_state=None,\n",
    "            verbose=0,\n",
    "            warm_start=False\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.train_data, self.train_labels, test_size=0.2, random_state=42)\n",
    "        # Fit the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "        val_predictions = classifier.predict(X_val) \n",
    "        # Save model\n",
    "        joblib.dump(classifier,self.path_save_model + \"random_forest_grid_search_hyperparameters.joblib\")\n",
    "        cv_scores = cross_val_score(classifier, self.train_data, self.train_labels, cv=5)\n",
    "        print(\"Cross-validation scores:\", cv_scores)\n",
    "        print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "        val_metrics = self.calculate_evaluation_metrics(y_val, val_predictions, \"Validation\")\n",
    "        test_predictions = classifier.predict(self.test_data)\n",
    "        test_metrics = self.calculate_evaluation_metrics(self.test_labels, test_predictions, \"Test\")\n",
    "        return classifier\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    #SVM Classifier\n",
    "    def svm_classifier(self):\n",
    "        classifier = SVC()\n",
    "        cv_scores = cross_val_score(classifier, self.train_data, self.train_labels, cv=5)\n",
    "        print(\"Cross-validation scores:\", cv_scores)\n",
    "        print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.train_data, self.train_labels, test_size=0.2, random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        joblib.dump(classifier,self.path_save_model + \"svm_classifier.joblib\")\n",
    "        val_metrics = self.calculate_evaluation_metrics(y_val, val_predictions, \"Validation\")\n",
    "        test_predictions = classifier.predict(self.test_data)\n",
    "        test_metrics = self.calculate_evaluation_metrics(self.test_labels, test_predictions, \"Test\")\n",
    "        return classifier\n",
    "    \n",
    "    #Finding Hyperparameters SVM Grid\n",
    "    # def svm_grid_search(self):\n",
    "    #     param_grid = {\n",
    "    #         'C': [0.1, 1, 10],\n",
    "    #         'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    #         'gamma': [0.1, 1, 'scale']\n",
    "    #     }\n",
    "    #     classifier = SVC()\n",
    "    #     grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "    #     X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "    # \n",
    "    #     grid_search.fit(self.train_data, self.train_labels)\n",
    "    #     best_classifier = grid_search.best_estimator_\n",
    "    #     print(\"Selected Hyperparameters:\", best_classifier.get_params(), \"\\n\\n\")\n",
    "    #     cv_scores = cross_val_score(best_classifier, self.train_data, self.train_labels, cv=5)\n",
    "    #     print(\"Cross-validation scores:\", cv_scores)\n",
    "    #     print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "    #     best_classifier.fit(self.train_data, self.train_labels)\n",
    "    #     val_predictions = best_classifier.predict(self.X_val)\n",
    "    #     joblib.dump(best_classifier,\"ActivityMonitoring/svm_grid_search.joblib\")\n",
    "    #     val_metrics = self.calculate_evaluation_metrics(self.y_val, val_predictions, \"Validation\")\n",
    "    #     test_predictions = best_classifier.predict(self.test_data)\n",
    "    #     test_metrics = self.calculate_evaluation_metrics(self.test_labels, test_predictions, \"Test\")\n",
    "    #     return best_classifier\n",
    "    \n",
    "    #SVM Grid with trained hyperparameters\n",
    "    def svm_grid_search_hyperparameters(self):\n",
    "        classifier = SVC(\n",
    "            C=0.1,\n",
    "            break_ties=False,\n",
    "            cache_size=200,\n",
    "            class_weight=None,\n",
    "            coef0=0.0,\n",
    "            decision_function_shape='ovr',\n",
    "            degree=3,\n",
    "            gamma=0.1,\n",
    "            kernel='linear',\n",
    "            max_iter=-1,\n",
    "            probability=False,\n",
    "            random_state=None,\n",
    "            shrinking=True,\n",
    "            tol=0.001,\n",
    "            verbose=False,\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.train_data, self.train_labels, test_size=0.2, random_state=42)\n",
    "        # Fit the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "        val_predictions = classifier.predict(X_val) \n",
    "        # Save model\n",
    "        joblib.dump(classifier,self.path_save_model + \"svm_grid_search_hyperparameters.joblib\")\n",
    "        cv_scores = cross_val_score(classifier, self.train_data, self.train_labels, cv=5)\n",
    "        print(\"Cross-validation scores:\", cv_scores)\n",
    "        print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "        val_metrics = self.calculate_evaluation_metrics(y_val, val_predictions, \"Validation\")\n",
    "        test_predictions = classifier.predict(self.test_data)\n",
    "        test_metrics = self.calculate_evaluation_metrics(self.test_labels, test_predictions, \"Test\")\n",
    "        return classifier\n",
    "        \n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    data_dir = 'ActivityMonitoring/'\n",
    "    train_data = load_data(\"training\")\n",
    "    test_data = load_data(\"testing\")\n",
    "    train_labels = np.load(data_dir + 'training/trainLabels.npy')\n",
    "    test_labels = np.load(data_dir + 'testing/testLabels.npy')\n",
    "\n",
    "    data_loader = ActivityMonitor(train_data, test_data, train_labels, test_labels)\n",
    "    data_loader.preprocess_data()\n",
    "\n",
    "    # Random Forest Classifier\n",
    "    random_forest_clf = data_loader.random_forest_classifier()\n",
    "    random_forest_clf_grid = data_loader.random_forest_grid_search_hyperparameters()\n",
    "\n",
    "    # SVM Classifier\n",
    "    svm_clf = data_loader.svm_classifier()\n",
    "    svm_grid_clf = data_loader.svm_grid_search_hyperparameters()\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T11:59:55.395875300Z",
     "start_time": "2023-11-15T11:52:52.056872600Z"
    }
   },
   "id": "322f0aa8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54483379422a857"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def predict_activity(inp, model_no):\n",
    "    path_save_model = \"ActivityMonitoring/TrainedModels/\"\n",
    "    activity_labels = {\n",
    "        0: \"Bring\",\n",
    "        1: \"Clean Floor\",\n",
    "        2: \"Clean Surface\",\n",
    "        3: \"Close Big Box\",\n",
    "        4: \"Close Door\",\n",
    "        5: \"Close Drawer\",\n",
    "        6: \"Close Lid By Rotate\",\n",
    "        7: \"Close Other Lid\",\n",
    "        8: \"Close Small Box\",\n",
    "        9: \"Close Tap Water\",\n",
    "        10: \"Drink\",\n",
    "        11: \"Dry Off Hands\",\n",
    "        12: \"Dry Off Hands By Shake\",\n",
    "        13: \"Eat Small\",\n",
    "        14: \"Gargle\",\n",
    "        15: \"Getting Up\",\n",
    "        16: \"Hang\",\n",
    "        17: \"Lying Down\",\n",
    "        18: \"Open Bag\",\n",
    "        19: \"Open Big Box\",\n",
    "        20: \"Open Door\",\n",
    "        21: \"Open Drawer\",\n",
    "        22: \"Open Lid By Rotate\",\n",
    "        23: \"Open Other Lid\",\n",
    "        24: \"Open Small Box\",\n",
    "        25: \"Open Tap Water\",\n",
    "        26: \"Plug In\",\n",
    "        27: \"Press By Grasp\",\n",
    "        28: \"Press From Top\",\n",
    "        29: \"Press Switch\",\n",
    "        30: \"Put From Bottle\",\n",
    "        31: \"Put From Tap Water\",\n",
    "        32: \"Put High Position\",\n",
    "        33: \"Put On Floor\",\n",
    "        34: \"Read\",\n",
    "        35: \"Rotate\",\n",
    "        36: \"Rub Hands\",\n",
    "        37: \"Scoop And Put\",\n",
    "        38: \"Sitting Down\",\n",
    "        39: \"Squatting Down\",\n",
    "        40: \"Standing Up\",\n",
    "        41: \"Stand Up From Squatting\",\n",
    "        42: \"Take From Floor\",\n",
    "        43: \"Take From High Position\",\n",
    "        44: \"Take Off Jacket\",\n",
    "        45: \"Take Out\",\n",
    "        46: \"Talk By Telephone\",\n",
    "        47: \"Throw Out\",\n",
    "        48: \"Throw Out Water\",\n",
    "        49: \"Touch Smartphone Screen\",\n",
    "        50: \"Type\",\n",
    "        51: \"Unhang\",\n",
    "        52: \"Unplug\",\n",
    "        53: \"Wear Jacket\",\n",
    "        54: \"Write\"\n",
    "    }\n",
    "    \n",
    "    classifier = joblib.load(path_save_model + \"random_forest_classifier.joblib\")\n",
    "    \n",
    "    if(model_no == \"Random Forest\"):\n",
    "        classifier = joblib.load(path_save_model + \"random_forest_classifier.joblib\")    \n",
    "    elif(model_no == \"Random Forest Grid\"):\n",
    "        classifier = joblib.load(path_save_model + \"random_forest_grid_search_hyperparameters.joblib\")\n",
    "    elif(model_no == \"SVM\"):\n",
    "        classifier = joblib.load(path_save_model + \"svm_classifier.joblib\")\n",
    "    elif(model_no == \"SVM Grid\"):\n",
    "        classifier = joblib.load(path_save_model + \"svm_grid_search_hyperparameters.joblib\")\n",
    "    \n",
    "    #arr = [data_loader.test_data[int(inp), :]]\n",
    "    true_label = inp[0]\n",
    "    arr = inp[1:]\n",
    "\n",
    "    np_arr = np.array(arr)\n",
    "    #print(np_arr.shape)\n",
    "    \n",
    "    prediction = classifier.predict(np_arr.reshape(1, -1))\n",
    "    \n",
    "    #print(prediction[0])\n",
    "        \n",
    "    activity = activity_labels.get(prediction[0])\n",
    "    \n",
    "    print(activity)\n",
    "    true_label_concat = \"Label = \" + str(true_label) + \"\\tActivity = \" + activity_labels.get(true_label)\n",
    "    predicted_label_concat = \"Label = \" + str(prediction[0]) + \"\\tActivity = \" + activity\n",
    "    \n",
    "    return true_label_concat, predicted_label_concat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T05:47:55.158714700Z",
     "start_time": "2023-11-19T05:47:54.981319300Z"
    }
   },
   "id": "f6c5e5f58b5e7139"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict_wrapper(csv_file_path, model):\n",
    "    data = np.genfromtxt(csv_file_path, delimiter=',')    \n",
    "    return predict_activity(data, model)\n",
    "\n",
    "activity_prediction = gr.Interface(\n",
    "    fn=predict_wrapper,  # Pass your function\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload CSV File\"),\n",
    "        #gr.Textbox(type=\"text\", label=\"Model No\", placeholder=\"Enter model to test\")\n",
    "        gr.Radio(choices=[\"Random Forest\", \"Random Forest Grid\", \"SVM\", \"SVM Grid\"], value=\"Random Forest\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(type=\"text\", label=\"True Label\"),\n",
    "        gr.Textbox(type=\"text\", label=\"Prediction\")\n",
    "    ]  # Output component\n",
    ")\n",
    "activity_prediction.launch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T05:48:01.199502700Z",
     "start_time": "2023-11-19T05:47:55.161715600Z"
    }
   },
   "id": "4f8be0d7b36e9793"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gender Recognition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0e62da8badefba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1965d215b29feb5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m         y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenderRecognition/labels.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n\u001B[1;32m---> 20\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msplit_data\u001B[39m(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, valid_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m):\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m# split training set and testing set\u001B[39;00m\n\u001B[0;32m     24\u001B[0m     X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39mtest_size, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7\u001B[39m)\n",
      "Cell \u001B[1;32mIn[3], line 15\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m(vector_length)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A function to load gender recognition dataset from `data` folder\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03mAfter the second run, this will load from results/features.npy and results/labels.npy files\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03mas it is much faster!\"\"\"\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# make sure results folder exists\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m#if not os.path.isdir(\"results\"):\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m#    os.mkdir(\"results\")\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# if features & labels already loaded individually and bundled, load them from there instead\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenderRecognition/features.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenderRecognition/labels.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     16\u001B[0m     X \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenderRecognition/features.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     17\u001B[0m     y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenderRecognition/labels.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "import pickle\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    \"\"\"A function to load gender recognition dataset from `data` folder\n",
    "    After the second run, this will load from results/features.npy and results/labels.npy files\n",
    "    as it is much faster!\"\"\"\n",
    "    # make sure results folder exists\n",
    "    #if not os.path.isdir(\"results\"):\n",
    "    #    os.mkdir(\"results\")\n",
    "    # if features & labels already loaded individually and bundled, load them from there instead\n",
    "    if os.path.isfile(\"GenderRecognition/features.npy\") and os.path.isfile(\"GenderRecognition/labels.npy\"):\n",
    "        X = np.load(\"GenderRecognition/features.npy\")\n",
    "        y = np.load(\"GenderRecognition/labels.npy\")\n",
    "        return X, y\n",
    "    \n",
    "X, y = load_data()\n",
    "\n",
    "def split_data(X, y, test_size=0.1, valid_size=0.1):\n",
    "    # split training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n",
    "    # split training set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n",
    "    # return a dictionary of values\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_data()\n",
    "# split the data into training, validation and testing sets\n",
    "data = split_data(X, y, test_size=0.1, valid_size=0.1)\n",
    "\n",
    "def create_model(vector_length=128):\n",
    "    \"\"\"5 hidden dense layers from 256 units to 64, not the best model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# construct the model\n",
    "model = create_model()\n",
    "\n",
    "# use tensorboard to view metrics\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "# define early stopping to stop training after 5 epochs of not improving\n",
    "early_stopping = EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "# train the model using the training set and validating using validation set\n",
    "model.fit(data[\"X_train\"], data[\"y_train\"], epochs=epochs, batch_size=batch_size, validation_data=(data[\"X_valid\"], data[\"y_valid\"]),\n",
    "          callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "# save the model to a file\n",
    "#model.save(\"results/model.h5\")\n",
    "pickle.dump(model, open(\"GenderRecognition/TrainedModels/gender.pickle\", 'wb'))\n",
    "\n",
    "\n",
    "# evaluating the model using the testing set\n",
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")#6694\n",
    "loss, accuracy = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "#0.2143\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "#92.02%\n",
    "\n",
    "def conf_matrix():\n",
    "    # Create the confusion matrix values\n",
    "    cm = confusion_matrix(data[\"y_test\"][:10000], svm_predictions)\n",
    "\n",
    "    # Create the confusion matrix display\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('Confusion matrix on test data')\n",
    "    sns.heatmap(cm, annot=True, fmt='d', \n",
    "                cmap=plt.cm.Blues, cbar=False, annot_kws={'size':14})\n",
    "    #to visualise a confusion matrix, time-series movements, temperature changes, correlation matrix and SHAP interaction values\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:54:07.708498300Z",
     "start_time": "2023-11-17T05:54:03.663071200Z"
    }
   },
   "id": "3aeb03d0e90bc2d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6bec3f9effe497a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import librosa\n",
    "import pickle\n",
    "import os\n",
    "\"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X)) #analyze the frequency content of a signal over time\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "args = parser.parse_args()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T05:48:03.138886700Z",
     "start_time": "2023-11-19T05:48:03.095718200Z"
    }
   },
   "id": "5c2a09f55159f782"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def test_sample(file):\n",
    "    path_to_model = \"GenderRecognition/TrainedModels/\"\n",
    "    # if not file or not os.path.isfile(file):\n",
    "    #     return \"file doesn't exist\"\n",
    "    # else:\n",
    "    loaded_model = pickle.load(open(path_to_model+\"gender.pickle\", 'rb'))\n",
    "    features = extract_feature(file, mel=True).reshape(1, -1)\n",
    "    # predict the gender!\n",
    "    print(loaded_model.predict(features))\n",
    "    male_prob = loaded_model.predict(features)[0][0]\n",
    "    female_prob = 1 - male_prob\n",
    "    gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "    # show the result!\n",
    "    #print(\"Result:\", gender)\n",
    "    #print(f\"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")\n",
    "    result = \"Result: \" + gender + \", Probabilities::: Male: \" + f\"{male_prob*100:.2f}\" + \"   Female: \" + f\"{female_prob*100:.2f}\"\n",
    "    print(result)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T05:48:04.564496300Z",
     "start_time": "2023-11-19T05:48:04.535445500Z"
    }
   },
   "id": "89ce7bd3a8a51475"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def gender_recoginition(test_example):\n",
    "    return test_sample(test_example)\n",
    "\n",
    "gender_prediction_with_file = gr.Interface(\n",
    "    fn = gender_recoginition,    \n",
    "    inputs = [\n",
    "        gr.Audio(sources=[\"upload\"], show_download_button=True, format=\"wav\", type=\"filepath\")\n",
    "        #gr.File(label=\"Upload audio file...\")\n",
    "    ],\n",
    "    outputs = \"text\",\n",
    "    live=True,\n",
    ")\n",
    "gender_prediction_with_file.launch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T05:48:06.040199Z",
     "start_time": "2023-11-19T05:48:05.780597300Z"
    }
   },
   "id": "5f3c709ce260a7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hunger Detection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40107a8f16394305"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "139631699c0bf580"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def DetectHunger(main_dir, data, model):\n",
    "    # data = pd.read_csv(file)    \n",
    "\n",
    "    X = data.drop('Label', axis=1)\n",
    "    y = data['Label']\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    if(model == \"RF_15\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/randomforest_15_interval_model.joblib\")\n",
    "    elif(model == \"RF_30\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/randomforest_30_interval_model.joblib\")\n",
    "    elif(model == \"RF_60\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/randomforest_60_interval_model.joblib\")\n",
    "        \n",
    "    elif(model == \"SVM_15\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/svm_15_interval_model.joblib\")\n",
    "    elif(model == \"SVM_30\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/svm_30_interval_model.joblib\")\n",
    "    elif(model == \"SVM_60\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/svm_60_interval_model.joblib\")\n",
    "        \n",
    "    elif(model == \"NB_15\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/naivebayes_15_interval_model.joblib\")\n",
    "    elif(model == \"NB_30\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/naivebayes_30_interval_model.joblib\")\n",
    "    elif(model == \"NB_60\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/naivebayes_60_interval_model.joblib\")\n",
    "        \n",
    "    elif(model == \"LR_15\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/logisticregression_15_interval_model.joblib\")\n",
    "    elif(model == \"LR_30\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/logisticregression_30_interval_model.joblib\")\n",
    "    elif(model == \"LR_60\"):\n",
    "        model = joblib.load(f\"{main_dir}TrainedModels/logisticregression_60_interval_model.joblib\")\n",
    "        \n",
    "    prediction = model.predict(X)\n",
    "    pred = \"\"\n",
    "    \n",
    "    if(y.iloc[0] == 0):\n",
    "        pred = \"Not Hungry\"\n",
    "    elif(y.iloc[0] == 1):\n",
    "        pred = \"Hungry\"\n",
    "    \n",
    "    true_label = f\"Label={y.iloc[0]}\\tPrediction={pred}\"\n",
    "       \n",
    "    if(prediction[0] == 0):\n",
    "        pred = \"Not Hungry\"\n",
    "    elif(prediction[0] == 1):\n",
    "        pred = \"Hungry\"\n",
    "    \n",
    "    predicted = f\"Label={prediction[0]}\\tPrediction={pred}\" \n",
    "    \n",
    "    return true_label, predicted\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T05:56:40.697797900Z",
     "start_time": "2023-11-19T05:56:40.675463300Z"
    }
   },
   "id": "4ab567753953008f"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "('Label=0\\tPrediction=Not Hungry', 'Label=1\\tPrediction=Hungry')"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dir = \"HungerDetection/\"\n",
    "file_path = f\"{main_dir}TestExamples/t4.csv\"\n",
    "file = pd.read_csv(file_path)\n",
    "DetectHunger(main_dir, file, \"NB_60\")\n",
    "#type(true_label)\n",
    "#type(prediction[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T05:57:02.465937800Z",
     "start_time": "2023-11-19T05:57:02.417147600Z"
    }
   },
   "id": "979c1372ec1205fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combined "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "423a78c01906e621"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UniversityData\\PycharmProjects\\venv\\lib\\site-packages\\gradio\\blocks.py:946: UserWarning: api_name predict already exists, using predict_1\n",
      "  warnings.warn(\n",
      "D:\\UniversityData\\PycharmProjects\\venv\\lib\\site-packages\\gradio\\blocks.py:946: UserWarning: api_name predict already exists, using predict_2\n",
      "  warnings.warn(\n",
      "D:\\UniversityData\\PycharmProjects\\venv\\lib\\site-packages\\gradio\\blocks.py:946: UserWarning: api_name predict already exists, using predict_3\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://9140ea2fb86826a223.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"https://9140ea2fb86826a223.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def predict_wrapper(csv_file_path, model):\n",
    "    data = np.genfromtxt(csv_file_path, delimiter=',')\n",
    "    return predict_activity(data, model)\n",
    "\n",
    "def gender_recoginition(test_example):\n",
    "    return test_sample(test_example)\n",
    "\n",
    "def detect_Wrapper(csv_file_path, model):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    main_dir = \"HungerDetection/\"\n",
    "    return DetectHunger(main_dir, data, model)\n",
    "\n",
    "activity_prediction = gr.Interface(\n",
    "    fn=predict_wrapper,  # Pass your function\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload CSV File\"),\n",
    "        #gr.Textbox(type=\"text\", label=\"Model No\", placeholder=\"Enter model to test\")\n",
    "        gr.Radio(choices=[\"Random Forest\", \"Random Forest Grid\", \"SVM\", \"SVM Grid\"], value=\"Random Forest\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(type=\"text\", label=\"True Label\"),\n",
    "        gr.Textbox(type=\"text\", label=\"Prediction\")\n",
    "    ]  # Output component\n",
    ")\n",
    "\n",
    "hunger_detection = gr.Interface(\n",
    "    fn=detect_Wrapper,  # Pass your function\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload CSV File\"),\n",
    "        #gr.Textbox(type=\"text\", label=\"Model No\", placeholder=\"Enter model to test\")\n",
    "        gr.Radio(choices=[\n",
    "            \"RF_15\", \"RF_30\", \"RF_60\", \n",
    "            \"SVM_15\", \"SVM_30\", \"SVM_60\", \n",
    "            \"LR_15\", \"LR_30\", \"LR_60\",\n",
    "            \"NB_15\", \"NB_30\", \"NB_60\"\n",
    "        ], value=\"RF_15\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(type=\"text\", label=\"True Label\"),\n",
    "        gr.Textbox(type=\"text\", label=\"Prediction\")\n",
    "    ]  # Output component\n",
    ")\n",
    "\n",
    "gender_prediction_with_file = gr.Interface(\n",
    "    fn = gender_recoginition,    \n",
    "    inputs = [\n",
    "        gr.Audio(sources=[\"upload\"], show_download_button=True, format=\"wav\", type=\"filepath\")\n",
    "        #gr.File(label=\"Upload audio file...\")\n",
    "    ],\n",
    "    outputs = \"text\",\n",
    "    live=True,\n",
    ")\n",
    "\n",
    "\n",
    "gender_prediction_realtime = gr.Interface(\n",
    "    fn = gender_recoginition,    \n",
    "    inputs = [\n",
    "        gr.Audio(sources=[\"microphone\"], show_download_button=True, format=\"wav\", type=\"filepath\")\n",
    "    ],\n",
    "    outputs = \"text\",\n",
    "    live=True,\n",
    ")\n",
    "\n",
    "group = gr.TabbedInterface([activity_prediction,hunger_detection,gender_prediction_with_file,gender_prediction_realtime],tab_names=[\"Activity Recognition\", \"Hunger Detection\", \"Gender Classification With File\", \"Gender Classification Realtime\"])\n",
    "\n",
    "# Launch the group interface\n",
    "group.launch(share=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T05:57:18.865026400Z",
     "start_time": "2023-11-19T05:57:12.448814300Z"
    }
   },
   "id": "9758cb506b0a9d22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9f889bf2cb7a130b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
